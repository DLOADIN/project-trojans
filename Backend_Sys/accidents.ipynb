{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Define constants\n",
    "BATCH_SIZE = 32\n",
    "IMG_HEIGHT = 250  # Match the exact dimensions used in camera.py\n",
    "IMG_WIDTH = 250   # Match the exact dimensions used in camera.py\n",
    "EPOCHS = 50\n",
    "\n",
    "# Data augmentation for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Only rescaling for validation and test\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Load datasets\n",
    "train_data = train_datagen.flow_from_directory(\n",
    "    'data/train',\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',  # Changed to categorical\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "validation_data = valid_datagen.flow_from_directory(\n",
    "    'data/val',\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',  # Changed to categorical\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_data = test_datagen.flow_from_directory(\n",
    "    'data/test',\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',  # Changed to categorical\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "def build_model():\n",
    "    model = models.Sequential([\n",
    "        # Initial normalization\n",
    "        layers.BatchNormalization(input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n",
    "        \n",
    "        # First convolutional block\n",
    "        layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Second convolutional block\n",
    "        layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Third convolutional block\n",
    "        layers.Conv2D(128, 3, padding='same', activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(128, 3, padding='same', activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Dense layers\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(2, activation='softmax')  # 2 classes: Accident and Non-Accident\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create model\n",
    "model = build_model()\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Create callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.2,\n",
    "    patience=5,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    \"model_weights_enhanced.h5\",\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train model\n",
    "history = model.fit(\n",
    "    train_data,\n",
    "    validation_data=validation_data,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[checkpoint, early_stopping, reduce_lr]\n",
    ")\n",
    "\n",
    "# Evaluate model\n",
    "test_loss, test_accuracy = model.evaluate(test_data)\n",
    "print(f\"\\nTest accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Generate predictions\n",
    "predictions = model.predict(test_data)\n",
    "y_pred = np.argmax(predictions, axis=1)\n",
    "y_true = test_data.classes\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    target_names=['Non-Accident', 'Accident']\n",
    "))\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save model architecture to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model_enhanced.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# Save model weights\n",
    "model.save_weights(\"model_weights_enhanced.weights.h5\")\n",
    "\n",
    "print(\"\\nModel has been saved as 'model_enhanced.json' and 'model_weights_enhanced.weights.h5'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
